{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiVPT7a/jbQTGC/oZ78cku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jarpit2003/Advance_Python/blob/main/Advance_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "23jMFuN9xaAe"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "with open('random_strings.txt', 'w') as file:\n",
        "    for _ in range(1000):\n",
        "        random_string = generate_random_string(50)\n",
        "        file.write(random_string + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "with open('5mb_file.txt', 'w') as file:\n",
        "    while file.tell() < 5 * 1024 * 1024:  # 5 MB in bytes\n",
        "        random_string = generate_random_string(100)\n",
        "        file.write(random_string + '\\n')"
      ],
      "metadata": {
        "id": "u93ElwiexgIo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "import os\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "def create_5mb_file(filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        while file.tell() < 5 * 1024 * 1024:  # 5 MB in bytes\n",
        "            random_string = generate_random_string(100)\n",
        "            file.write(random_string + '\\n')\n",
        "\n",
        "for i in range(10):\n",
        "    create_5mb_file(f'5mb_file_{i+1}.txt')"
      ],
      "metadata": {
        "id": "GbAGy7YKxkj3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_string(length):\n",
        "    return ''.join(random.choices(string.ascii_letters + string.digits, k=length))\n",
        "\n",
        "def create_large_file(filename, size_gb):\n",
        "    with open(filename, 'w') as file:\n",
        "        while file.tell() < size_gb * 1024 * 1024 * 1024:  # Convert GB to bytes\n",
        "            random_string = generate_random_string(1000)\n",
        "            file.write(random_string + '\\n')\n",
        "\n",
        "for size in range(1, 6):\n",
        "    create_large_file(f'{size}GB_file.txt', size)"
      ],
      "metadata": {
        "id": "_wFX3q8FxmyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "def convert_to_uppercase(input_file, output_file):\n",
        "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            outfile.write(line.upper())\n",
        "\n",
        "threads = []\n",
        "for size in range(1, 6):\n",
        "    input_file = f'{size}GB_file.txt'\n",
        "    output_file = f'{size}GB_file_uppercase_threaded.txt'\n",
        "    thread = threading.Thread(target=convert_to_uppercase, args=(input_file, output_file))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "for thread in threads:\n",
        "    thread.join()"
      ],
      "metadata": {
        "id": "DVkjHEkWxpPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "\n",
        "arguments = {\n",
        "    \"keywords\": \"cat\",\n",
        "    \"limit\": 10,\n",
        "    \"print_urls\": True\n",
        "}\n",
        "\n",
        "paths = response.download(arguments)"
      ],
      "metadata": {
        "id": "vVVgSUtYxzqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "from pytube import Search\n",
        "\n",
        "# Search for \"Machine Learning\" videos\n",
        "s = Search(\"Machine Learning\")\n",
        "\n",
        "# Download the first 10 videos\n",
        "for i, video in enumerate(s.results[:10]):\n",
        "    try:\n",
        "        yt = YouTube(f\"https://www.youtube.com/watch?v={video.video_id}\")\n",
        "        yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()\n",
        "        print(f\"Downloaded video {i+1}\")\n",
        "    except:\n",
        "        print(f\"Failed to download video {i+1}\")"
      ],
      "metadata": {
        "id": "zRCL4xbIx40q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def video_to_audio(video_file, audio_file):\n",
        "    video = VideoFileClip(video_file)\n",
        "    video.audio.write_audiofile(audio_file)\n",
        "\n",
        "# Convert all mp4 files in the current directory\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".mp4\"):\n",
        "        video_file = file\n",
        "        audio_file = file[:-4] + \".mp3\"\n",
        "        video_to_audio(video_file, audio_file)"
      ],
      "metadata": {
        "id": "oJD2UGO5x7yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from pytube import YouTube, Search\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def download_and_convert(video_id, index):\n",
        "    try:\n",
        "        yt = YouTube(f\"https://www.youtube.com/watch?v={video_id}\")\n",
        "        video_file = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download()\n",
        "        audio_file = f\"audio_{index}.mp3\"\n",
        "\n",
        "        video = VideoFileClip(video_file)\n",
        "        video.audio.write_audiofile(audio_file)\n",
        "\n",
        "        print(f\"Processed video {index}\")\n",
        "    except:\n",
        "        print(f\"Failed to process video {index}\")\n",
        "\n",
        "# Search for videos\n",
        "s = Search(\"Machine Learning\")\n",
        "\n",
        "# Create and start threads\n",
        "threads = []\n",
        "for i, video in enumerate(s.results[:100]):\n",
        "    thread = threading.Thread(target=download_and_convert, args=(video.video_id, i+1))\n",
        "    threads.append(thread)\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for thread in threads:\n",
        "    thread.join()"
      ],
      "metadata": {
        "id": "44Tifxnkx_mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create dataset\n",
        "df = pd.DataFrame(np.random.randint(1, 201, size=(100, 30)))\n",
        "\n",
        "# (i) Replace values with NA\n",
        "df[(df >= 10) & (df <= 60)] = np.nan\n",
        "print(f\"Number of rows with missing values: {df.isnull().any(axis=1).sum()}\")\n",
        "\n",
        "# (ii) Replace NA with column average\n",
        "df = df.fillna(df.mean())\n",
        "\n",
        "# (iii) Pearson correlation and heatmap\n",
        "corr = df.corr()\n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
        "plt.title('Pearson Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "# Select columns with correlation <= 0.7\n",
        "high_corr_cols = np.where(np.abs(corr) > 0.7)\n",
        "high_corr_cols = [(corr.index[x], corr.columns[y]) for x, y in zip(*high_corr_cols) if x != y and x < y]\n",
        "print(\"Columns with correlation <= 0.7:\")\n",
        "print(set(df.columns) - set([col for pair in high_corr_cols for col in pair]))\n",
        "\n",
        "# (iv) Normalize values between 0 and 10\n",
        "df_normalized = (df - df.min()) / (df.max() - df.min()) * 10\n",
        "\n",
        "# (v) Replace values with 1 if <= 0.5, else 0\n",
        "df_binary = (df_normalized <= 0.5).astype(int)"
      ],
      "metadata": {
        "id": "6x0biGzUyC0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create dataset\n",
        "df = pd.DataFrame({\n",
        "    **{f'col_{i}': np.random.uniform(-10, 10, 500) for i in range(1, 5)},\n",
        "    **{f'col_{i}': np.random.uniform(10, 20, 500) for i in range(5, 9)},\n",
        "    **{f'col_{i}': np.random.uniform(-100, 100, 500) for i in range(9, 11)}\n",
        "})\n",
        "\n",
        "# K-Means clustering\n",
        "kmeans_silhouette_scores = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(df)\n",
        "    score = silhouette_score(df, kmeans.labels_)\n",
        "    kmeans_silhouette_scores.append(score)\n",
        "\n",
        "plt.plot(k_range, kmeans_silhouette_scores)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('K-Means Clustering')\n",
        "plt.show()\n",
        "\n",
        "# Hierarchical clustering\n",
        "hierarchical_silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    hierarchical = AgglomerativeClustering(n_clusters=k)\n",
        "    hierarchical.fit(df)\n",
        "    score = silhouette_score(df, hierarchical.labels_)\n",
        "    hierarchical_silhouette_scores.append(score)\n",
        "\n",
        "plt.plot(k_range, hierarchical_silhouette_scores)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Hierarchical Clustering')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ZzlowsnyGK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create dataset\n",
        "df = pd.DataFrame(np.random.uniform(-100, 100, size=(600, 15)))\n",
        "\n",
        "# (i) Scatter plot between Column 5 and Column 6\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df[4], df[5])\n",
        "plt.xlabel('Column 5')\n",
        "plt.ylabel('Column 6')\n",
        "plt.title('Scatter plot: Column 5 vs Column 6')\n",
        "plt.show()\n",
        "\n",
        "# (ii) Histogram of each column\n",
        "df.hist(figsize=(20, 15))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# (iii) Box plot of each column\n",
        "df.boxplot(figsize=(20, 10))\n",
        "plt.title('Box plot of all columns')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l2jShQP8yIy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Create dataset\n",
        "df = pd.DataFrame(np.random.uniform(5, 10, size=(500, 5)))\n",
        "\n",
        "# (i) Perform t-Test on each column\n",
        "for col in df.columns:\n",
        "    t_stat, p_value = stats.ttest_1samp(df[col], 7.5)  # testing against the mean of the range\n",
        "    print(f\"Column {col}: t-statistic = {t_stat}, p-value = {p_value}\")\n",
        "\n",
        "# (ii) Perform Wilcoxon Signed Rank Test on each column\n",
        "for col in df.columns:\n",
        "    statistic, p_value = stats.wilcoxon(df[col] - 7.5)  # testing against the mean of the range\n",
        "    print(f\"Column {col}: statistic = {statistic}, p-value = {p_value}\")\n",
        "\n",
        "# (iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
        "t_stat, p_value = stats.ttest_ind(df[2], df[3])\n",
        "print(f\"Two Sample t-Test: t-statistic = {t_stat}, p-value = {p_value}\")\n",
        "\n",
        "statistic, p_value = stats.ranksums(df[2], df[3])\n",
        "print(f\"Wilcoxon Rank Sum Test: statistic = {statistic}, p-value = {p_value}\")"
      ],
      "metadata": {
        "id": "o1p463nJyMNY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}